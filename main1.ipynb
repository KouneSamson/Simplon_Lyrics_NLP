{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1>1# Exploration des Datas </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importer la bibliothèque numpy et lui donner l'alias `np`.\n",
    "import numpy as np\n",
    "\n",
    "# Importer la bibliothèque matplotlib et lui donner l'alias `plt`.\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             ALink                            SName  \\\n",
      "0  /ivete-sangalo/                            Arerê   \n",
      "1  /ivete-sangalo/  Se Eu Não Te Amasse Tanto Assim   \n",
      "2  /ivete-sangalo/                      Céu da Boca   \n",
      "3  /ivete-sangalo/            Quando A Chuva Passar   \n",
      "4  /ivete-sangalo/                     Sorte Grande   \n",
      "\n",
      "                                               SLink  \\\n",
      "0                          /ivete-sangalo/arere.html   \n",
      "1  /ivete-sangalo/se-eu-nao-te-amasse-tanto-assim...   \n",
      "2                     /ivete-sangalo/chupa-toda.html   \n",
      "3          /ivete-sangalo/quando-a-chuva-passar.html   \n",
      "4                   /ivete-sangalo/sorte-grande.html   \n",
      "\n",
      "                                               Lyric language  \n",
      "0  Tudo o que eu quero nessa vida,\\nToda vida, é\\...       pt  \n",
      "1  Meu coração\\nSem direção\\nVoando só por voar\\n...       pt  \n",
      "2  É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...       pt  \n",
      "3  Quando a chuva passar\\n\\nPra quê falar\\nSe voc...       pt  \n",
      "4  A minha sorte grande foi você cair do céu\\nMin...       pt  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lyrics_df = pd.read_csv(\"lyrics-data.csv\",sep=\",\",quotechar='\"')\n",
    "\n",
    "print(lyrics_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Artist                  Genres  Songs  Popularity  \\\n",
      "0        Ivete Sangalo     Pop; Axé; Romântico  313.0         4.4   \n",
      "1  Chiclete com Banana                     Axé  268.0         3.8   \n",
      "2            Banda Eva  Axé; Romântico; Reggae  215.0         2.3   \n",
      "3            É O Tchan                     Axé  129.0         1.6   \n",
      "4       Claudia Leitte     Pop; Axé; Romântico  167.0         1.5   \n",
      "\n",
      "                   ALink  \n",
      "0        /ivete-sangalo/  \n",
      "1  /chiclete-com-banana/  \n",
      "2            /banda-eva/  \n",
      "3            /e-o-tchan/  \n",
      "4       /claudia-leitte/  \n"
     ]
    }
   ],
   "source": [
    "artists_df = pd.read_csv(\"artists-data.csv\",sep=\",\")\n",
    "\n",
    "# on renomme la colonne Link pour qu'elle soit la même que la colonne ALink de notre premier dataframe pour une jointure ( les valeurs sont les mêmes initialement\n",
    "#  mais pas les noms des colonnes)\n",
    "artists_df.rename(columns = {'Link':'ALink'}, inplace = True)\n",
    "\n",
    "print(artists_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALink</th>\n",
       "      <th>SName</th>\n",
       "      <th>SLink</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genres</th>\n",
       "      <th>Songs</th>\n",
       "      <th>Popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Arerê</td>\n",
       "      <td>/ivete-sangalo/arere.html</td>\n",
       "      <td>Tudo o que eu quero nessa vida,\\nToda vida, é\\...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "      <td>Pop; Axé; Romântico</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Se Eu Não Te Amasse Tanto Assim</td>\n",
       "      <td>/ivete-sangalo/se-eu-nao-te-amasse-tanto-assim...</td>\n",
       "      <td>Meu coração\\nSem direção\\nVoando só por voar\\n...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "      <td>Pop; Axé; Romântico</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Céu da Boca</td>\n",
       "      <td>/ivete-sangalo/chupa-toda.html</td>\n",
       "      <td>É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "      <td>Pop; Axé; Romântico</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Quando A Chuva Passar</td>\n",
       "      <td>/ivete-sangalo/quando-a-chuva-passar.html</td>\n",
       "      <td>Quando a chuva passar\\n\\nPra quê falar\\nSe voc...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "      <td>Pop; Axé; Romântico</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/ivete-sangalo/</td>\n",
       "      <td>Sorte Grande</td>\n",
       "      <td>/ivete-sangalo/sorte-grande.html</td>\n",
       "      <td>A minha sorte grande foi você cair do céu\\nMin...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Ivete Sangalo</td>\n",
       "      <td>Pop; Axé; Romântico</td>\n",
       "      <td>313.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ALink                            SName  \\\n",
       "0  /ivete-sangalo/                            Arerê   \n",
       "1  /ivete-sangalo/  Se Eu Não Te Amasse Tanto Assim   \n",
       "2  /ivete-sangalo/                      Céu da Boca   \n",
       "3  /ivete-sangalo/            Quando A Chuva Passar   \n",
       "4  /ivete-sangalo/                     Sorte Grande   \n",
       "\n",
       "                                               SLink  \\\n",
       "0                          /ivete-sangalo/arere.html   \n",
       "1  /ivete-sangalo/se-eu-nao-te-amasse-tanto-assim...   \n",
       "2                     /ivete-sangalo/chupa-toda.html   \n",
       "3          /ivete-sangalo/quando-a-chuva-passar.html   \n",
       "4                   /ivete-sangalo/sorte-grande.html   \n",
       "\n",
       "                                               Lyric language         Artist  \\\n",
       "0  Tudo o que eu quero nessa vida,\\nToda vida, é\\...       pt  Ivete Sangalo   \n",
       "1  Meu coração\\nSem direção\\nVoando só por voar\\n...       pt  Ivete Sangalo   \n",
       "2  É de babaixá!\\nÉ de balacubaca!\\nÉ de babaixá!...       pt  Ivete Sangalo   \n",
       "3  Quando a chuva passar\\n\\nPra quê falar\\nSe voc...       pt  Ivete Sangalo   \n",
       "4  A minha sorte grande foi você cair do céu\\nMin...       pt  Ivete Sangalo   \n",
       "\n",
       "                Genres  Songs  Popularity  \n",
       "0  Pop; Axé; Romântico  313.0         4.4  \n",
       "1  Pop; Axé; Romântico  313.0         4.4  \n",
       "2  Pop; Axé; Romântico  313.0         4.4  \n",
       "3  Pop; Axé; Romântico  313.0         4.4  \n",
       "4  Pop; Axé; Romântico  313.0         4.4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# on fusionne les colonnes\n",
    "df = pd.merge(lyrics_df, artists_df, \n",
    "                   on='ALink', \n",
    "                   how='right')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378994, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A tuple that contains the number of rows and columns in the dataframe.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALink          object\n",
       "SName          object\n",
       "SLink          object\n",
       "Lyric          object\n",
       "language       object\n",
       "Artist         object\n",
       "Genres         object\n",
       "Songs         float64\n",
       "Popularity    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#affiche les types de variables de chaque  colonnes \n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALink             1\n",
       "SName             9\n",
       "SLink             7\n",
       "Lyric            83\n",
       "language      14592\n",
       "Artist            1\n",
       "Genres           40\n",
       "Songs             1\n",
       "Popularity        2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there are any null values in the dataframe.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on supprime les lignes qui ont des valeurs manquante\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALink         0\n",
       "SName         0\n",
       "SLink         0\n",
       "Lyric         0\n",
       "language      0\n",
       "Artist        0\n",
       "Genres        0\n",
       "Songs         0\n",
       "Popularity    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if there are any null values in the dataframe.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#On regarde si il y a des doublons\n",
    "df.duplicated(subset=None, keep='first').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 364374 entries, 0 to 378993\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count   Dtype  \n",
      "---  ------      --------------   -----  \n",
      " 0   ALink       364374 non-null  object \n",
      " 1   SName       364374 non-null  object \n",
      " 2   SLink       364374 non-null  object \n",
      " 3   Lyric       364374 non-null  object \n",
      " 4   language    364374 non-null  object \n",
      " 5   Artist      364374 non-null  object \n",
      " 6   Genres      364374 non-null  object \n",
      " 7   Songs       364374 non-null  float64\n",
      " 8   Popularity  364374 non-null  float64\n",
      "dtypes: float64(2), object(7)\n",
      "memory usage: 27.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Showing the data type of each column.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Songs</th>\n",
       "      <th>Popularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>364374.000000</td>\n",
       "      <td>364374.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>194.208308</td>\n",
       "      <td>4.481692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>162.020802</td>\n",
       "      <td>13.196156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>146.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>248.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1029.000000</td>\n",
       "      <td>205.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Songs     Popularity\n",
       "count  364374.000000  364374.000000\n",
       "mean      194.208308       4.481692\n",
       "std       162.020802      13.196156\n",
       "min         1.000000       0.000000\n",
       "25%        87.000000       0.000000\n",
       "50%       146.000000       1.000000\n",
       "75%       248.000000       3.000000\n",
       "max      1029.000000     205.500000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pop; Axé; Romântico' 'Axé' 'Axé; Romântico; Reggae' ...\n",
      " 'World Music; Indie; New Age' 'World Music; Gospel/Religioso'\n",
      " 'World Music; Black Music; Blues']\n"
     ]
    }
   ],
   "source": [
    "# afficher toute les valeurs uniques par colonne\n",
    "print(df['Genres'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pt' 'es' 'en' 'it' 'gl' 'fr' 'de' 'tl' 'et' 'fi' 'pl' 'da' 'st' 'sv'\n",
      " 'ro' 'af' 'no' 'eu' 'rw' 'sw' 'ga' 'cy' 'ca' 'ny' 'ko' 'ar' 'gd' 'tr'\n",
      " 'id' 'su' 'lg' 'ru' 'nl' 'sq' 'is' 'cs' 'jw' 'hu' 'ms' 'ku' 'zh' 'hr'\n",
      " 'ht' 'fa' 'mg' 'vi' 'ja' 'hmn' 'sr' 'iw' 'sl']\n"
     ]
    }
   ],
   "source": [
    "print(df['language'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en     191376\n",
       "pt     156940\n",
       "es       9916\n",
       "rw       1663\n",
       "it       1426\n",
       "fr       1221\n",
       "de        843\n",
       "fi        145\n",
       "sv        112\n",
       "ro         97\n",
       "no         89\n",
       "is         86\n",
       "tl         69\n",
       "pl         47\n",
       "gl         36\n",
       "ga         32\n",
       "tr         32\n",
       "id         26\n",
       "cy         23\n",
       "su         19\n",
       "af         19\n",
       "sw         19\n",
       "ko         17\n",
       "nl         14\n",
       "da         13\n",
       "ca         13\n",
       "et         12\n",
       "ms          8\n",
       "ja          7\n",
       "st          6\n",
       "ht          5\n",
       "ar          4\n",
       "ru          4\n",
       "eu          4\n",
       "gd          4\n",
       "cs          3\n",
       "ku          3\n",
       "ny          3\n",
       "mg          3\n",
       "lg          2\n",
       "jw          2\n",
       "hu          2\n",
       "iw          1\n",
       "sr          1\n",
       "hmn         1\n",
       "hr          1\n",
       "vi          1\n",
       "fa          1\n",
       "sq          1\n",
       "zh          1\n",
       "sl          1\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['language'].value_counts() # on additionne chaque ligne ayant la même valeur dans la colonne langage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Country                                  4774\n",
       "Rock                                     4672\n",
       "Heavy Metal                              4393\n",
       "Indie                                    4288\n",
       "Rap; Hip Hop                             3426\n",
       "                                         ... \n",
       "J-Pop/J-Rock; Rock; Trilha Sonora           1\n",
       "Surf Music; Reggae; Romântico               1\n",
       "Pop/Rock; Dance; Romântico                  1\n",
       "J-Pop/J-Rock; Trilha Sonora; Pop/Rock       1\n",
       "Reggaeton; Reggae; Axé                      1\n",
       "Name: Genres, Length: 1190, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# on selectionne les data qui ont des paroles en anglais car elles sont majoritaire on aura donc plus de data pour entrainer le modèle\n",
    "\n",
    "eng_text = df.loc[df['language'] == 'en']\n",
    "\n",
    "eng_text['Genres'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on selectionne le genre country qui est majoritaire \n",
    "\n",
    "dfCountry = df.loc[df['Genres'] == 'Country']\n",
    "\n",
    "# dfCountry = df.query('Genres == \"Country\"') pareil qu'au dessus mais avec query\n",
    "\n",
    "dfCountry = dfCountry['Lyric'].iloc[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyric = \"\" # chaine de caractère vide \n",
    "for lyric_data in dfCountry:\n",
    "    lyric += lyric_data + \"\\n\" #  l'operateur \" += \" fusionne les valeurs si c'est une chaine de caratère \n",
    "\n",
    "# print(lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  on converti tous les caractères en minuscules pour réduire le vocabulaire que le réseau doit apprendre.\n",
    "lyric =lyric.lower()\n",
    "\n",
    "# lyric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the book is loaded, you must prepare the data for modeling by the neural network. \n",
    "You cannot model the characters directly; instead, you must convert the characters to integers.\n",
    "\n",
    "You can do this easily by first creating a set of all of the distinct characters in the book,\n",
    "then creating a map of each character to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(lyric)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple, la liste des caractères minuscules triés uniques dans le livre est la suivante :\n",
    "\n",
    "['\\n', '\\r', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xbb', '\\xbf', '\\xef']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez voir qu'il peut y avoir des caractères que nous pourrions supprimer pour nettoyer davantage l'ensemble de données afin de réduire le vocabulaire, ce qui peut améliorer le processus de modélisation.\n",
    "\n",
    "Maintenant que le texte a été chargé et la cartographie préparée, vous pouvez résumer l'ensemble de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  526170\n",
      "Total Vocab:  85\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(lyric)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez voir que les paroles contiennent un peu moins de 500 000 caractères, et lorsqu'il est converti en minuscules, il n'y a que 32 caractères distincts dans le vocabulaire que le réseau doit apprendre, plus que les 26 de l'alphabet.\n",
    "\n",
    "Vous devez maintenant définir les données d'entraînement pour le réseau. Il y a beaucoup de flexibilité dans la façon dont vous choisissez de décomposer le texte et de l'exposer au réseau pendant la formation.\n",
    "\n",
    "Dans ce didacticiel, vous diviserez le texte du livre en sous-séquences d'une longueur fixe de 100 caractères, une longueur arbitraire. Vous pouvez tout aussi facilement diviser les données en phrases, en remplissant les séquences les plus courtes et en tronquant les plus longues.\n",
    "\n",
    "Chaque modèle d'apprentissage du réseau comprend 100 pas de temps d'un caractère (X) suivi d'une sortie de caractère (y). Lors de la création de ces séquences, vous faites glisser cette fenêtre le long du livre entier un caractère à la fois, laissant à chaque caractère une chance d'être appris parmi les 100 caractères qui l'ont précédé (sauf les 100 premiers caractères, bien sûr).\n",
    "\n",
    "Par exemple, si la longueur de la séquence est de 5 (pour simplifier), les deux premiers modèles d'apprentissage seraient les suivants :\n",
    "\n",
    "CHAPT -> E\n",
    "HAPTE -> R\n",
    "\n",
    "Au fur et à mesure que vous divisez le livre en ces séquences, vous convertissez les caractères en nombres entiers à l'aide de la table de correspondance que vous avez préparée précédemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  526070\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "# Slicing the lyric into chunks of 100 characters.\n",
    "\tseq_in = lyric[i:i + seq_length]\n",
    "\tseq_out = lyric[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'exécution du code à ce stade montre que lorsque vous divisez l'ensemble de données en données d'entraînement pour que le réseau apprenne que vous avez un peu moins de 500 000 modèles d'entraînement. Cela a du sens car, à l'exclusion des 100 premiers caractères, vous disposez d'un modèle d'apprentissage pour prédire chacun des caractères restants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que vous avez préparé vos données d'entraînement, vous devez les transformer pour qu'elles puissent être utilisées avec Keras.\n",
    "\n",
    "Tout d'abord, vous devez transformer la liste des séquences d'entrée sous la forme [échantillons, pas de temps, caractéristiques] attendue par un réseau LSTM.\n",
    "\n",
    "Ensuite, vous devez redimensionner les entiers dans la plage de 0 à 1 pour faciliter l'apprentissage des modèles par le réseau LSTM en utilisant la fonction d'activation sigmoïde par défaut.\n",
    "\n",
    "Enfin, vous devez convertir les modèles de sortie (caractères simples convertis en entiers) en un encodage à chaud. C'est ainsi que vous pouvez configurer le réseau pour prédire la probabilité de chacun des 118 caractères différents dans le vocabulaire (une représentation plus facile) plutôt que d'essayer de le forcer à prédire précisément le caractère suivant. Chaque valeur y est convertie en un vecteur clairsemé d'une longueur de 118, plein de zéros, sauf avec un 1 dans la colonne de la lettre (entier) que le motif représente.\n",
    "\n",
    "Par exemple, lorsque \"n\" (valeur entière 31) est codé à chaud, il se présente comme suit :\n",
    "\n",
    "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
    "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
    "  0.  0.  0.  0.  0.  0.  0.  0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant définir votre modèle LSTM. Ici, vous définissez une seule couche LSTM cachée avec 256 unités de mémoire. Le réseau utilise l'abandon avec une probabilité de 20. La couche de sortie est une couche Dense utilisant la fonction d'activation softmax pour produire une prédiction de probabilité pour chacun des 118 caractères entre 0 et 1.\n",
    "\n",
    "Le problème est en réalité un problème de classification à caractère unique avec 118 classes et, en tant que tel, est défini comme l'optimisation de la perte de log (entropie croisée) à l'aide de l'algorithme d'optimisation ADAM pour la vitesse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the LSTM model\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il n'y a pas d'ensemble de données de test. Vous modélisez l'intégralité de l'ensemble de données d'entraînement pour connaître la probabilité de chaque caractère dans une séquence.\n",
    "\n",
    "Vous n'êtes pas intéressé par le modèle le plus précis (précision de la classification) de l'ensemble de données d'entraînement. Ce serait un modèle qui prédit parfaitement chaque caractère dans l'ensemble de données d'apprentissage. Au lieu de cela, vous êtes intéressé par une généralisation de l'ensemble de données qui minimise la fonction de perte choisie. Vous recherchez un équilibre entre la généralisation et le surapprentissage mais à court de mémorisation.\n",
    "\n",
    "Le réseau est lent à s'entraîner (environ 300 secondes par époque sur un GPU Nvidia K520). En raison de la lenteur et des exigences d'optimisation, utilisez le point de contrôle du modèle pour enregistrer tous les poids du réseau à enregistrer chaque fois qu'une amélioration de la perte est observée à la fin de l'époque. Vous utiliserez le meilleur ensemble de pondérations (perte la plus faible) pour instancier votre modèle génératif dans la section suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez maintenant ajuster votre modèle aux données. Ici, vous utilisez un nombre modeste de 20 époques et une grande taille de lot de 128 modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8218/8220 [============================>.] - ETA: 0s - loss: 2.6686 - accuracy: 0.2383\n",
      "Epoch 1: loss improved from inf to 2.66856, saving model to weights-improvement-01-2.6686-bigger.hdf5\n",
      "8220/8220 [==============================] - 122s 15ms/step - loss: 2.6686 - accuracy: 0.2383\n",
      "Epoch 2/50\n",
      "8219/8220 [============================>.] - ETA: 0s - loss: 2.3214 - accuracy: 0.3206\n",
      "Epoch 2: loss improved from 2.66856 to 2.32138, saving model to weights-improvement-02-2.3214-bigger.hdf5\n",
      "8220/8220 [==============================] - 116s 14ms/step - loss: 2.3214 - accuracy: 0.3206\n",
      "Epoch 3/50\n",
      "8220/8220 [==============================] - ETA: 0s - loss: 2.1431 - accuracy: 0.3690\n",
      "Epoch 3: loss improved from 2.32138 to 2.14307, saving model to weights-improvement-03-2.1431-bigger.hdf5\n",
      "8220/8220 [==============================] - 111s 13ms/step - loss: 2.1431 - accuracy: 0.3690\n",
      "Epoch 4/50\n",
      "8220/8220 [==============================] - ETA: 0s - loss: 2.0293 - accuracy: 0.4007\n",
      "Epoch 4: loss improved from 2.14307 to 2.02934, saving model to weights-improvement-04-2.0293-bigger.hdf5\n",
      "8220/8220 [==============================] - 111s 14ms/step - loss: 2.0293 - accuracy: 0.4007\n",
      "Epoch 5/50\n",
      "8219/8220 [============================>.] - ETA: 0s - loss: 1.9462 - accuracy: 0.4229\n",
      "Epoch 5: loss improved from 2.02934 to 1.94615, saving model to weights-improvement-05-1.9462-bigger.hdf5\n",
      "8220/8220 [==============================] - 118s 14ms/step - loss: 1.9462 - accuracy: 0.4229\n",
      "Epoch 6/50\n",
      "8220/8220 [==============================] - ETA: 0s - loss: 1.8827 - accuracy: 0.4405\n",
      "Epoch 6: loss improved from 1.94615 to 1.88270, saving model to weights-improvement-06-1.8827-bigger.hdf5\n",
      "8220/8220 [==============================] - 113s 14ms/step - loss: 1.8827 - accuracy: 0.4405\n",
      "Epoch 7/50\n",
      "8219/8220 [============================>.] - ETA: 0s - loss: 1.8320 - accuracy: 0.4533\n",
      "Epoch 7: loss improved from 1.88270 to 1.83196, saving model to weights-improvement-07-1.8320-bigger.hdf5\n",
      "8220/8220 [==============================] - 112s 14ms/step - loss: 1.8320 - accuracy: 0.4533\n",
      "Epoch 8/50\n",
      "8219/8220 [============================>.] - ETA: 0s - loss: 1.7890 - accuracy: 0.4664\n",
      "Epoch 8: loss improved from 1.83196 to 1.78898, saving model to weights-improvement-08-1.7890-bigger.hdf5\n",
      "8220/8220 [==============================] - 111s 14ms/step - loss: 1.7890 - accuracy: 0.4664\n",
      "Epoch 9/50\n",
      "8219/8220 [============================>.] - ETA: 0s - loss: 1.7510 - accuracy: 0.4772\n",
      "Epoch 9: loss improved from 1.78898 to 1.75101, saving model to weights-improvement-09-1.7510-bigger.hdf5\n",
      "8220/8220 [==============================] - 111s 14ms/step - loss: 1.7510 - accuracy: 0.4772\n",
      "Epoch 10/50\n",
      "8217/8220 [============================>.] - ETA: 0s - loss: 1.7214 - accuracy: 0.4852\n",
      "Epoch 10: loss improved from 1.75101 to 1.72136, saving model to weights-improvement-10-1.7214-bigger.hdf5\n",
      "8220/8220 [==============================] - 111s 13ms/step - loss: 1.7214 - accuracy: 0.4852\n",
      "Epoch 11/50\n",
      "8219/8220 [============================>.] - ETA: 0s - loss: 1.6949 - accuracy: 0.4928\n",
      "Epoch 11: loss improved from 1.72136 to 1.69490, saving model to weights-improvement-11-1.6949-bigger.hdf5\n",
      "8220/8220 [==============================] - 109s 13ms/step - loss: 1.6949 - accuracy: 0.4928\n",
      "Epoch 12/50\n",
      "8220/8220 [==============================] - ETA: 0s - loss: 1.6719 - accuracy: 0.4987\n",
      "Epoch 12: loss improved from 1.69490 to 1.67192, saving model to weights-improvement-12-1.6719-bigger.hdf5\n",
      "8220/8220 [==============================] - 105s 13ms/step - loss: 1.6719 - accuracy: 0.4987\n",
      "Epoch 13/50\n",
      "8218/8220 [============================>.] - ETA: 0s - loss: 1.6523 - accuracy: 0.5052\n",
      "Epoch 13: loss improved from 1.67192 to 1.65235, saving model to weights-improvement-13-1.6523-bigger.hdf5\n",
      "8220/8220 [==============================] - 105s 13ms/step - loss: 1.6523 - accuracy: 0.5052\n",
      "Epoch 14/50\n",
      "8220/8220 [==============================] - ETA: 0s - loss: 1.6323 - accuracy: 0.5098\n",
      "Epoch 14: loss improved from 1.65235 to 1.63227, saving model to weights-improvement-14-1.6323-bigger.hdf5\n",
      "8220/8220 [==============================] - 117s 14ms/step - loss: 1.6323 - accuracy: 0.5098\n",
      "Epoch 15/50\n",
      "7824/8220 [===========================>..] - ETA: 6s - loss: 1.6178 - accuracy: 0.5140"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cb176315d1b334180b6af9fe4839184c771682056334f245fd0d7c99b98cf46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
